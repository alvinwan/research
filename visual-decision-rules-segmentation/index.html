<html>
  <head>
    <title>SegNBDT: Visual Decision Rules for Segmentation | Alvin Wan | Efficient Machine Learning Researcher at UC Berkeley</title>
    <script src="../static/script.js"></script>
    <link href="../static/v2.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,400;0,500;0,600;0,700&display=swap" rel="stylesheet">
  </head>
  <body>
    <header class="dark wrap-container">
      <div class="wrap-wide">
        <a class="button" href="..">
          <i class="fas fa-arrow-left"></i>
          <span>Back</span>
        </a>
      </div>
    </header>
    <section class="hero dark wrap-container">
      <div class="wrap-wide">
        <div class="card" style="background-image: url('../images/card-fire.svg')">
          <img src="../images/decision-tree.svg">
        </div>
        <div class="hero-text">
          <h1>Visual Decision Rules for Segmentation</h1>
          <p class="small"><span>Alvin Wan*, Daniel Ho*, Younjin Song, Henk Tillman, Sarah Adel Bargal, Joseph E. Gonzalez</span></p>
          <p>Our models, <b>Segmentation Neural-Backed Decision Trees</b> (SegNBDT), are competitive with state-of-the-art neural networks on segmentation and feature <i>visual decision rules</i>.</p>
          <div class="buttons">
            <a class="button" href="https://arxiv.org/abs/2006.06868">
              <i class="fas fa-sticky-note"></i>
              <span>Paper</span>
            </a>
            <a class="button" href="https://github.com/daniel-ho/SegNBDT">
              <i class="fas fa-code"></i>
              <span>Code</span>
            </a>
          </div>
        </div>
      </div>
    </section>
    <section class="blog meta wrap-container">
      <div class="wrap text">
        <div class="blog-row">
          <div class="blog-col">
            <h3>Authors</h3>
            <p>Alvin Wan*, Daniel Ho*, Younjin Song, Henk Tillman, Sarah Adel Bargal, Joseph E. Gonzalez</p>
            <p><span>*denotes equal contribution</span></p>
          </div>
        </div>
        <div class="blog-row">
          <div class="blog-col">
            <h3>Affiliations</h3>
            <p>University of California, Berkeley</p>
            <p>Boston University</p>
          </div>
          <div class="blog-col">
            <h3>Published (Preprint)</h3>
            <p>June 11, 2020</p>
          </div>
        </div>
      </div>
    </section>
    <section class="blog wrap-container">
      <div class="wrap text">
        <h1>Abstract</h1>
        <p>The black-box nature of neural networks limits model decision interpretability, in particular for high-dimensional inputs in computer vision and for dense pixel prediction tasks like segmentation. To address this, prior work combines neural networks with decision trees. However, such models</p>
        <ol>
          <li>perform poorly when compared to state-of-the-art segmentation models</li>
          <li>fail to produce decision rules with spatially-grounded semantic meaning</li>
        </ol>
        <p>In this work, we build a hybrid neural-network and decision-tree model for segmentation that (1) attains neural network segmentation accuracy and (2) provides semi-automatically constructed visual decision rules such as "Is there a window?". We obtain semantic visual meaning by extending saliency methods to segmentation and attain accuracy by leveraging insights from neural-backed decision trees, a deep learning analog of decision trees for image classification. Our model SegNBDT attains accuracy within ~2-4% of the state-of-the-art HRNetV2 segmentation model while also retaining explainability; we achieve state-of-the-art performance for explainable models on three benchmark datasets -- Pascal-Context (49.12%), Cityscapes (79.01%), and Look Into Person (51.64%). Furthermore, user studies suggest visual decision rules are more interpretable, particularly for incorrect predictions. Code and pretrained models can be found on <a href="https://github.com/daniel-ho/SegNBDT">Github</a>.</p>
        <h1>Takeaways</h1>
        <p>Our work culminates in three key contributions that you can takeaway for future research:</p>
        <ul>
          <li>We build SegNBDT, achieving accuracy within 2% of state-of-the-art neural network HRNetV2 on Cityscapes, maintaining interpretable properties. This shows <b>accuracy and interpretability can be jointly attained <i>even for segmentation</i>.</b></li>
          <li>We tailor popular classification saliency methods to segmentation. We then leverage segmentation labels, sometimes from a different dataset, to <b>associate saliency with semantic image parts to understand general model behavior</b>.</li>
          <li>Understanding model mistakes is challenging: Saliency maps offer no insight when a model looks at the right object. For this reason, ~80% of <b>users prefer visual, semantic decision rules over a single saliency map</b> when assessing a misclassification.</li>
        </ul>
        <div class="buttons">
          <a class="button" href="https://arxiv.org/abs/2006.06868">
            <i class="fas fa-sticky-note"></i>
            <span>Paper</span>
          </a>
          <!-- <a class="button" href="https://towardsdatascience.com/what-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07?source=friends_link&sk=f7ad769e8b830ff128be4612ca2f146b">
            <i class="fas fa-rss"></i>
            <span>Blog</span>
          </a> -->
          <a class="button" href="https://github.com/daniel-ho/SegNBDT">
            <i class="fas fa-code"></i>
            <span>Code</span>
          </a>
        </div>
      </div>
    </section>
    <footer class="blog meta wrap-container">
      <div class="wrap text">
        <h3>Ack</h3>
        <p>In addition to NSF CISE Expeditions Award CCF-1730628, UC Berkeley research is supported by gifts from Alibaba, Amazon Web Services, Ant Financial, CapitalOne, Ericsson, Facebook, Futurewei, Google, Intel, Microsoft, Nvidia, Scotiabank, Splunk and VMware. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE 1752814.</p>
        <h3>Citation</h3>
        <p>BibTeX citation</p>
        <code><pre>@misc{segnbdt,
    title={SegNBDT: Visual Decision Rules for Segmentation},
    author={Alvin Wan and Daniel Ho and Younjin Song and Henk Tillman and Sarah Adel Bargal and Joseph E. Gonzalez},
    year={2020},
    eprint={2006.06868},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}</pre></code>
        <p><br/>Website theme designed by <a href="https://alvinwan.com">Alvin</a>. Publications list generated from Google Scholar using <a href="https://github.com/alvinwan/webfscholar">webfscholar</a>.</p>
      </div>
    </footer>
    <script src="https://kit.fontawesome.com/67993e702b.js" crossorigin="anonymous"></script>
    <!-- Google Analytics -->
    <script>
      (function(i, s, o, g, r, a, m)
      {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function()
        {
          (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o),
          m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
      })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
      ga('create', 'UA-32800241-3', 'alvinwan.com');
      ga('send', 'pageview');
    </script>
  </body>
</html>
