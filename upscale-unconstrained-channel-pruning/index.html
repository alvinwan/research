<html>
  <head>
    <title>UPSCALE: Unconstrained Channel Pruning | Alvin Wan | Efficient Deep Learning Researcher</title>
    <script src="../static/script.js"></script>
    <link href="../static/v2.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,400;0,500;0,600;0,700&display=swap" rel="stylesheet">
  </head>
  <body>
    <header class="dark wrap-container">
      <div class="wrap-wide">
        <a class="button" href="..">
          <i class="fas fa-arrow-left"></i>
          <span>Back</span>
        </a>
      </div>
    </header>
    <section class="hero dark wrap-container">
      <div class="wrap-wide">
        <!-- <div class="card" style="background-image: url('../images/card-turqoise.svg')">
          <img src="../images/decision-tree.svg">
        </div> -->
        <div class="hero-text">
          <h1>Unconstrained Channel Pruning</h1>
          <p class="small"><span>Alvin Wan &middot; Hanxiang Hao &middot; Kaushik Patnaik &middot; Yueyang Xu &middot; Omer Hadad &middot; David G&uuml;era &middot; Zhile Ren &middot; Qi Shan</span></p>
          <p>Our export library, called <b>UPSCALE</b>, improves accuracy of any channel pruning algorithm by removing constraints.</p>
          <div class="buttons">
            <a class="button" href="https://arxiv.org/abs/2307.08771">
              <i class="fas fa-sticky-note"></i>
              <span>Paper</span>
            </a>
            <a class="button" href="https://icml.cc/virtual/2023/poster/25215">
              <i class="fas fa-rss"></i>
              <span>ICML</span>
            </a>
            <a class="button" href="https://github.com/apple/ml-upscale">
              <i class="fas fa-code"></i>
              <span>Code</span>
            </a>
          </div>
        </div>
      </div>
    </section>
    <section class="blog meta wrap-container">
      <div class="wrap text">
        <div class="blog-row">
          <div class="blog-col">
            <h3>Authors</h3>
            <p>Alvin Wan &middot; Hanxiang Hao &middot; Kaushik Patnaik &middot; Yueyang Xu &middot; Omer Hadad &middot; David G&uuml;era &middot; Zhile Ren &middot; Qi Shan</p>
          </div>
        </div>
        <div class="blog-row">
          <div class="blog-col">
            <h3>Affiliations</h3>
            <p>Apple</p>
          </div>
          <div class="blog-col">
            <h3>Publish Date</h3>
            <p>Jul 26, 2023 <span>Int'l Conference on Machine Learning (ICML)</span></p>
          </div>
        </div>
      </div>
    </section>
    <section class="blog wrap-container">
      <div class="wrap text">
        <!-- <iframe width="100%" height="450" src="https://www.youtube.com/embed/fQ2eNFCSRiA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        <iframe width="100%" height="450" style="margin-top:3em" src="https://www.youtube.com/embed/bC5n1Yov7D0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
        <img src="https://i.imgur.com/EVITOrC.png">
        <h1>Abstract</h1>
        <p>
          As neural networks grow in size and complexity, inference speeds decline. To combat this, one of the most effective compression techniques -- channel pruning -- removes channels from weights. However, for multi-branch segments of a model, channel removal can introduce inference-time memory copies. In turn, these copies increase inference latency -- so much so that the pruned model can be slower than the unpruned model.
        </p>
        <p>As a workaround, pruners conventionally constrain certain channels to be pruned together. This fully eliminates memory copies but, as we show, significantly impairs accuracy. We now have a dilemma: Remove constraints but increase latency, or add constraints and impair accuracy.</p>
        <p>
          In response, our insight is to reorder channels at export time, (1) reducing latency by reducing memory copies and (2) improving accuracy by removing constraints. Using this insight, we design a generic algorithm UPSCALE to prune models with any pruning pattern. By removing constraints from existing pruners, we improve ImageNet accuracy for post-training pruned models by 2.1 points on average -- benefiting DenseNet (+16.9), EfficientNetV2 (+7.9), and ResNet (+6.2). Furthermore, by reordering channels, UPSCALE improves inference speeds by up to 2x over a baseline export.
        </p>
        <p>Code is on <a href="https://github.com/apple/ml-upscale">Github</a>.</p>
        <!-- <h1>Takeaways</h1>
        <p>Our work culminates in three key contributions that you can takeaway for future research:</p>
        <ul>
          <li>Our Neural-Backed Decision Tree achieves accuracy competitive with then-state-of-the-art neural network EfficientNet on ImageNet, maintaining interpretable properties, showing <b>accuracy and interpretability can be jointly improved.</b></li>
          <li>Hierarchical softmax is known to hurt accuracy. We thus instead develop and use a Tree Supervision Loss, demonstrating <b>hierarchical softmax is not the only way to train a hierarchical classifier.</b></li>
          <li>Our Induced Hierarchy, built from the weights of a pretrained neural network, outperforms existing taxonomies (e.g., WordNet) and data-based hierarchies (e.g., using information gain), demonstrating <b>a hierarchy built-in weight space is most effective.</b></li>
        </ul>
        <div class="buttons">
          <a class="button" href="https://arxiv.org/abs/2004.00221">
            <i class="fas fa-sticky-note"></i>
            <span>Paper</span>
          </a>
          <a class="button" href="https://towardsdatascience.com/what-explainable-ai-fails-to-explain-and-how-we-fix-that-1e35e37bee07?source=friends_link&sk=f7ad769e8b830ff128be4612ca2f146b">
            <i class="fas fa-rss"></i>
            <span>Blog</span>
          </a>
        </div> -->
        <h1>Getting Started</h1>
        <p>Installation is just one line.</p>
        <code>pip install apple-upscale</code>
        <p>Run on any model of your choosing.</p>
        <code><pre>
  import torch, torchvision
  from upscale import MaskingManager, PruningManager
  
  x = torch.rand((1, 3, 224, 224), device='cuda')
  model = torchvision.models.get_model('resnet18', pretrained=True).cuda()  # get any pytorch model
  MaskingManager(model).importance().mask()
  PruningManager(model).compute([x]).prune()
</pre></code>
        <div class="buttons">
          <a class="button" href="https://colab.research.google.com/drive/1vTZBWB3O2oj-g8oH5sj7j4CJ_xe6mv1p?usp=sharing">
            <i class="fab fa-google"></i>
            <span>Colab</span>
          </a>
          <a class="button" href="https://github.com/apple/ml-upscale">
            <i class="fas fa-code"></i>
            <span>Code</span>
          </a>
        </div>
      </div>
    </section>
    <footer class="blog meta wrap-container">
      <div class="wrap text">
        <!-- <h3>Ack</h3>
        <p>This article was made possible by input from many individuals at Apple.</p> -->
        <h3>Citation</h3>
        <p>BibTeX citation</p>
        <code><pre>@inproceedings{wan2023upscale,
  title={UPSCALE: Unconstrained Channel Pruning},
  author={Wan, Alvin and Hao, Hanxiang and Patnaik, Kaushik and Xu, Yueyang and Hadad, Omer and G{\"u}era, David and Ren, Zhile and Shan, Qi},
  booktitle={International Conference on Machine Learning},
  pages={35384--35412},
  year={2023},
  organization={PMLR}
}</pre></code>
        <p><br/>Website theme designed by <a href="https://alvinwan.com">Alvin</a>. Publications list generated from Google Scholar using <a href="https://github.com/alvinwan/webfscholar">webfscholar</a>.</p>
      </div>
    </footer>
    <script src="https://kit.fontawesome.com/67993e702b.js" crossorigin="anonymous"></script>
<!-- Google Analytics -->
    <script>
      (function(i, s, o, g, r, a, m)
      {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function()
        {
          (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o),
          m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
      })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
      ga('create', 'UA-32800241-3', 'alvinwan.com');
      ga('send', 'pageview');
    </script>
  </body>
</html>
